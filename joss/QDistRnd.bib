@book{Nielsen-book,
  address =       {Cambridge, MA},
  author =        {M. A. Nielsen and I. L. Chuang},
  publisher =     {Cambridge Unive. Press},
  title =         {Quantum Computation and Quantum Infomation},
  year =          {2000},
}

@inproceedings{Tillich-Zemor-2009,
  author =        {Tillich, J.-P. and Z{\'e}mor, G.},
  booktitle =     {Proc. IEEE Int. Symp. Inf. Theory (ISIT)},
  month =         {June},
  pages =         {799-803},
  title =         {Quantum {LDPC} codes with positive rate and minimum
                   distance proportional to {$\sqrt{n}$}},
  year =          {2009},
  annote =        {},
  doi =           {10.1109/ISIT.2009.5205648},
}

@article{Zeng-Pryadko-2018,
  author =        {Zeng, Weilei and Pryadko, Leonid P.},
  journal =       {Phys. Rev. Lett.},
  month =         {Jun},
  pages =         {230501},
  publisher =     {American Physical Society},
  title =         {Higher-Dimensional Quantum Hypergraph-Product Codes
                   with Finite Rates},
  volume =        {122},
  year =          {2019},
  doi =           {10.1103/PhysRevLett.122.230501},
  url =           {https://link.aps.org/doi/10.1103/PhysRevLett.122.230501},
}

@article{Zeng-Pryadko-hprod-2020,
  author =        {Zeng, Weilei and Pryadko, Leonid P.},
  journal =       {Phys. Rev. A},
  pages =         {062402},
  title =         {Minimal distances for certain quantum product codes
                   and tensor products of chain complexes},
  volume =        {102},
  year =          {2020},
  annote =        {We use a map to quantum error-correcting codes and a
                   subspace projection to get lower bounds for minimal
                   homological distances in a tensor product of two
                   chain complexes of vector spaces over a finite field.
                   Homology groups of such a complex are described by
                   the K�nneth theorem. We give an explicit expression
                   for the distances when one of the complexes is a
                   linear map between two spaces. The codes in the
                   construction, subsystem product codes and their
                   gauge-fixed variants, generalize several known
                   families of quantum error-correcting codes.},
  doi =           {10.1103/PhysRevA.102.062402},
  url =           {https://link.aps.org/doi/10.1103/PhysRevA.102.062402},
}

@article{Kovalev-Pryadko-Hyperbicycle-2013,
  author =        {Kovalev, A. A. and Pryadko, L. P.},
  journal =       {Phys. Rev. A},
  month =         {July},
  pages =         {012311},
  title =         {Quantum {K}ronecker sum-product low-density
                   parity-check codes with finite rate},
  volume =        {88},
  year =          {2013},
  annote =        {We introduce an ansatz for quantum codes which gives
                   the hypergraph-product (generalized toric) codes by
                   Tillich and Z�mor and generalized bicycle codes by
                   MacKay et al. as limiting cases. The construction
                   allows for both the lower and the upper bounds on the
                   minimum distance; they scale as a square root of the
                   block length. Many thus defined codes have a finite
                   rate and limited-weight stabilizer generators, an
                   analog of classical low-density parity-check (LDPC)
                   codes. Compared to the hypergraph-product codes,
                   hyperbicycle codes generally have a wider range of
                   parameters; in particular, they can have a higher
                   rate while preserving the estimated error threshold.},
  doi =           {10.1103/PhysRevA.88.012311},
  url =           {http://link.aps.org/doi/10.1103/PhysRevA.88.012311},
}

@unpublished{Bravyi-Hastings-2013,
  author =        {Sergey Bravyi and Matthew B. Hastings},
  note =          {unpublished},
  title =         {Homological Product Codes},
  year =          {2013},
  annote =        {Quantum codes with low-weight stabilizers known as
                   LDPC codes have been actively studied recently due to
                   their simple syndrome readout circuits and potential
                   applications in fault-tolerant quantum computing.
                   However, all families of quantum LDPC codes known to
                   this date suffer from a poor distance scaling limited
                   by the square-root of the code length. This is in a
                   sharp contrast with the classical case where good
                   families of LDPC codes are known that combine
                   constant encoding rate and linear distance. Here we
                   propose the first family of good quantum codes with
                   low-weight stabilizers. The new codes have a constant
                   encoding rate, linear distance, and stabilizers
                   acting on at most $\sqrt{n}$ qubits, where $n$ is the
                   code length. For comparison, all previously known
                   families of good quantum codes have stabilizers of
                   linear weight. Our proof combines two techniques:
                   randomized constructions of good quantum codes and
                   the homological product operation from algebraic
                   topology. We conjecture that similar methods can
                   produce good stabilizer codes with stabilizer weight
                   $n^a$ for any $a>0$. Finally, we apply the
                   homological product to construct new small codes with
                   low-weight stabilizers.},
}

@article{Guth-Lubotzky-2014,
  author =        {Guth, L. and Lubotzky, A.},
  journal =       {Journal of Mathematical Physics},
  number =        {8},
  pages =         {082202},
  title =         {Quantum error correcting codes and 4-dimensional
                   arithmetic hyperbolic manifolds},
  volume =        {55},
  year =          {2014},
  annote =        {Using 4-dimensional arithmetic hyperbolic manifolds,
                   we construct some new homological quantum error
                   correcting codes. They are low density parity check
                   codes with linear rate and distance n Îµ. Their
                   rate is evaluated via Euler characteristic arguments
                   and their distance using Z2Z2 -systolic geometry.
                   This construction answers a question of Zemor [``On
                   Cayley graphs, surface codes, and the limits of
                   homological coding for quantum error correction,'' in
                   Proceedings of Second International Workshop on
                   Coding and Cryptology (IWCC), (Lecture Notes in
                   Computer Science). Vol. 5557 (2009), pp. 259-273],
                   who asked whether homological codes with such
                   parameters could exist at all.},
  doi =           {https://doi.org/10.1063/1.4891487},
  url =           {http://scitation.aip.org/content/aip/journal/jmp/55/8/
                  10.1063/1.4891487},
}

@unpublished{Panteleev-Kalachev-2019,
  author =        {Pavel Panteleev and Gleb Kalachev},
  note =          {unpublished},
  title =         {Degenerate Quantum {LDPC} Codes With Good Finite
                   Length Performance},
  year =          {2019},
  annote =        {We study the performance of small and medium length
                   quantum LDPC (QLDPC) codes in the depolarizing
                   channel. Only degenerate codes with the maximal
                   stabilizer weight much smaller than their minimum
                   distance are considered. It is shown that with the
                   help of an OSD-like post-processing the performance
                   of the standard belief propagation (BP) decoder on
                   many QLDPC codes can be improved by several orders of
                   magnitude. Using this new BP-OSD decoder we study the
                   performance of several known classes of degenerate
                   QLDPC codes including the hypergraph product codes,
                   the hyperbicycle codes, the homological product
                   codes, and the Haah's cubic codes. We also construct
                   several interesting examples of short generalized
                   bicycle codes. Some of them have an additional
                   property that their syndromes are protected by small
                   BCH codes, which may be useful for the fault-tolerant
                   syndrome measurement. We also propose a new large
                   family of QLDPC codes that contains the class of
                   hypergraph product codes, where one of the used
                   parity-check matrices is square. It is shown that in
                   some cases such codes have better performance than
                   the hypergraph product codes. Finally, we demonstrate
                   that the performance of the proposed BP-OSD decoder
                   for some of the constructed codes is better than for
                   a relatively large surface code decoded by a
                   near-optimal decoder.},
}

@article{Evseev-1983,
  author =        {G. S. Evseev},
  journal =       {Probl. Peredachi Informacii},
  note =          {(In Russian)},
  pages =         {3-8},
  title =         {Complexity of decoding for linear codes.},
  volume =        {19},
  year =          {1983},
  annote =        {https://www.ams.org/mathscinet-getitem?mr=734143 The
                   author introduces the notion of $Q$ decoding of
                   linear codes. To each code is associated a $Q$
                   ensemble which is the product space of the Euclidean
                   space of dimension $n$ with the set of all
                   information ensembles of code $A$. Associated to the
                   $Q$ ensemble is a decoding algorithm. If the $Q$
                   ensemble of the linear code $A$ contains the coset
                   leaders of $A$ then $Q$ decoding is the same as
                   maximum likelihood decoding. Error probability of $Q$
                   decoding is upper bounded by 2 times the error
                   probability of maximum likelihood decoding. In part 4
                   the author proves the following theorem on the
                   complexity of $Q$ decoding: ``There exists a $Q$
                   decoding in a DSC such that for almost all linear
                   codes of length $n$ and rate $R$ the following holds:
                   $C_Q\le 2^{n\cdot(R(1-R)+O(1))},\;P_\varepsilon\le
                   2\cdot P_{\varepsilon_0}$.'' The proofs in the
                   English translation of the paper are too short to be
                   fully understood.},
  url =           {http://mi.mathnet.ru/ppi1159},
}

@article{Iyer-Poulin-2013,
  author =        {Pavithran Iyer and David Poulin},
  journal =       {{IEEE} Transactions on Information Theory},
  number =        {9},
  pages =         {5209-5223},
  title =         {Hardness of Decoding Quantum Stabilizer Codes},
  volume =        {61},
  year =          {2015},
  abstract =      {In this paper, we address the computational hardness
                   of optimally decoding a quantum stabilizer code. Much
                   like classical linear codes, errors are detected by
                   measuring certain check operators which yield an
                   error syndrome, and the decoding problem consists of
                   determining the most likely recovery given the
                   syndrome. The corresponding classical problem is
                   known to be NP-complete, and are appropriate a
                   similar decoding problem for quantum codes is also
                   known to be NP-complete. However, this decoding
                   strategy is not optimal in the quantum setting as it
                   does not consider error degeneracy, which causes
                   distinct errors to have the same effect on the code.
                   Here, we show that optimal decoding of stabilizer
                   codes (previously known to be NP-hard) is in fact
                   computationally much harder than optimal decoding of
                   classical linear codes, it is #P-complete.},
  doi =           {10.1109/TIT.2015.2422294},
  issn =          {1557-9654},
}

@article{magma-system,
  author =        {Bosma, Wieb and Cannon, John and Playoust, Catherine},
  journal =       {J. Symbolic Comput.},
  note =          {Computational algebra and number theory (London,
                   1993)},
  number =        {3-4},
  pages =         {235--265},
  title =         {The {M}agma algebra system. {I}. {T}he user language},
  volume =        {24},
  year =          {1997},
  annote =        {recommended citation for Magma},
  doi =           {10.1006/jsco.1996.0125},
  issn =          {0747-7171},
  url =           {https://doi.org/10.1006/jsco.1996.0125},
}

@article{Kovalev-Pryadko-FT-2013,
  author =        {Kovalev, A. A. and Pryadko, L. P.},
  journal =       {Phys. Rev. A},
  month =         {Feb},
  pages =         {020304(R)},
  publisher =     {American Physical Society},
  title =         {Fault tolerance of quantum low-density parity check
                   codes with sublinear distance scaling},
  volume =        {87},
  year =          {2013},
  annote =        {We discuss error-correction properties for families
                   of quantum low-density parity check (LDPC) codes with
                   relative distance that tends to zero in the limit of
                   large blocklength. In particular, we show that any
                   family of LDPC codes, quantum or classical, where
                   distance scales as a positive power of the block
                   length, $d \propto n^\alpha$, $\alpha>0$, can correct
                   all errors with certainty if the error rate per
                   (qu)bit is sufficiently small. We specifically
                   analyze the case of LDPC version of the quantum
                   hypergraph-product codes recently suggested by
                   Tillich and Z\'emor. These codes are a finite-rate
                   generalization of the toric codes, and, for
                   sufficiently large quantum computers, offer an
                   advantage over the toric codes.},
  doi =           {10.1103/PhysRevA.87.020304},
  url =           {http://link.aps.org/doi/10.1103/PhysRevA.87.020304},
}

@phdthesis{Breuckmann-thesis-2017,
  author =        {N. P. Breuckmann},
  school =        {RWTH Aachen University},
  title =         {Homological quantum codes beyond the toric code},
  year =          {2017},
  annote =        {Computer architectures which exploit quantum
                   mechanical effects can solve computing tasks that are
                   otherwise impossible to perform. A quantum computer
                   operates on a number of small quantum mechanical
                   systems, known as quantum bits, or qubits. Since
                   these systems are realized on the scale of atoms,
                   they are very prone to errors. Errors occur when the
                   environment interacts with the qubits, a process
                   called decoherence. It is widely accepted that it
                   will not be possible to shield qubits completely from
                   the outside world. If one were to perform a quantum
                   computation on the qubits directly, then after a
                   short period of time the information present in the
                   qubits would be lost. To counter decoherence the
                   state of a qubit can be encoded into multiple
                   physical ones. This is called a quantum error
                   correcting code. Performing quantum error correction
                   allows one to extend the life time of the encoded
                   qubit arbitrarily, assuming that the rate of errors
                   remains below a certain threshold value. The use of
                   quantum codes creates an overhead in resources, as
                   for every logical qubit many more physical qubits are
                   needed. The resource overhead for fault-tolerance is
                   problematic, since realizing qubits will be costly,
                   and in the early stages of building quantum computers
                   the number of physical qubits will be limited. The
                   currently favored coding architecture is the toric
                   code and its variant the surface code in which the
                   physical qubits are put on a square grid in which
                   interactions are only between nearest neighbors. In
                   this thesis we will explore quantum codes in which
                   qubits interact as if they were nearest neighbors in
                   more exotic spaces. In the first part we will
                   consider closed surfaces with constant negative
                   curvature. We show how such surfaces can be
                   constructed and enumerate all quantum codes derived
                   from them which have less than 10.000 physical
                   qubits. For codes that are extremal in a certain
                   sense we perform numerical simulations to determine
                   the value of their threshold. Furthermore, we give
                   evidence that these codes can be used for more
                   overhead efficient storage as compared to the surface
                   code by orders of magnitude. We also show how to read
                   and write the encoded qubits while keeping their
                   connectivity low. In the second part we consider
                   codes in which qubits are layed-out according to a
                   four- dimensional geometry. Such codes allow for much
                   simpler decoding schemes compared to codes which are
                   two-dimensional. In particular, measurements do not
                   necessarily have to be repeated to obtain reliable
                   information about the error and the classical
                   hardware performing the error correction is greatly
                   simplified. We perform numerical simulations to
                   analyze the performance of these codes using decoders
                   based on local updates. We also introduce a novel
                   decoder based on techniques from machine learning and
                   image recognition to decode four-dimensional codes.},
  doi =           {10.18154/RWTH-2018-01100},
}

@inproceedings{Hu-Fossorier-Eleftheriou-2004,
  author =        {Xiao-Yu Hu and Fossorier, M. P. C. and
                   Eleftheriou, E.},
  booktitle =     {Communications, 2004 IEEE International Conference
                   on},
  month =         {June},
  pages =         {767-771},
  title =         {On the computation of the minimum distance of
                   low-density parity-check codes},
  volume =        {2},
  year =          {2004},
  abstract =      {Low-density parity-check (LDPC) codes in their
                   broader-sense definition are linear codes whose
                   parity-check matrices have fewer 1s than 0s. Finding
                   their minimum distance is therefore in general an
                   NP-hard problem. We propose a randomized algorithm
                   called nearest nonzero codeword search (NNCS)
                   approach to tackle this problem for iteratively
                   decodable LDPC codes. The principle of the NNCS
                   approach is to search codewords locally around the
                   all-zero codeword perturbed by minimal noise,
                   anticipating that the resultant nearest nonzero
                   codewords will most likely contain the
                   minimum-Hamming- weight codeword whose Hamming weight
                   is equal to the minimum distance of the linear code.
                   This approach has its roots in Berrou et al.'s
                   error-impulse method and a form of Fossorier's list
                   decoding for LDPC codes.},
  doi =           {10.1109/ICC.2004.1312605},
}

@inproceedings{Declercq-Fossorier-2008,
  author =        {Declercq, D. and Fossorier, M.},
  booktitle =     {Information Theory, 2008. ISIT 2008. IEEE
                   International Symposium on},
  month =         {July},
  pages =         {1963-1967},
  title =         {Improved impulse method to evaluate the low weight
                   profile of sparse binary linear codes},
  year =          {2008},
  abstract =      {In this paper, the impulse method to determine the
                   low weight profile of sparse codes is improved based
                   on efficient probabilistic approaches for reliability
                   based decoding that are adapted to this problem. As a
                   result, compared with previous approaches, the same
                   low weight profile can be obtained with a significant
                   time reduction (for example from 30 hours to a few
                   minutes) or more complete low weight profiles can be
                   determined in the same amount of time.},
  doi =           {10.1109/ISIT.2008.4595332},
}

@article{Dumer-Kovalev-Pryadko-IEEE-2017,
  author =        {I. Dumer and A. A. Kovalev and L. P. Pryadko},
  journal =       {IEEE Trans. Inf. Th.},
  month =         {July},
  number =        {7},
  pages =         {4675-4686},
  title =         {Distance Verification for Classical and Quantum
                   {LDPC} Codes},
  volume =        {63},
  year =          {2017},
  abstract =      {The techniques of distance verification known for
                   general linear codes are first applied to the quantum
                   stabilizer codes. Then, these techniques are
                   considered for classical and quantum (stabilizer)
                   low-density-parity-check (LDPC) codes. New complexity
                   bounds for distance verification with provable
                   performance are derived using the average weight
                   spectra of the ensembles of LDPC codes. These bounds
                   are expressed in terms of the erasure-correcting
                   capacity of the corresponding ensemble. We also
                   present a new irreducible-cluster technique that can
                   be applied to any LDPC code and takes advantage of
                   parity-checks' sparsity for both the classical and
                   quantum LDPC codes. This technique reduces complexity
                   exponents of all existing deterministic techniques
                   designed for generic stabilizer codes with small
                   relative distances, which also include all known
                   families of the quantum stabilizer LDPC codes.},
  doi =           {10.1109/TIT.2017.2690381},
  issn =          {0018-9448},
}

@article{Leon-1988,
  author =        {Leon, J. S.},
  journal =       {IEEE Trans. Info. Theory},
  month =         {Sep},
  number =        {5},
  pages =         {1354 -1359},
  title =         {A probabilistic algorithm for computing minimum
                   weights of large error-correcting codes},
  volume =        {34},
  year =          {1988},
  abstract =      {An algorithm is developed that can be used to find,
                   with a very low probability of error (10-100 or less
                   in many cases), the minimum weights of codes far too
                   large to be treated by any known exact algorithm. The
                   probabilistic method is used to find minimum weights
                   of all extended quadratic residue codes of length 440
                   or less. The probabilistic algorithm is presented for
                   binary codes, but it can be generalized to codes over
                   GF(q) with q gt;2},
  doi =           {10.1109/18.21270},
  issn =          {0018-9448},
}

@article{Kruk-1989,
  author =        {E. A. Kruk},
  journal =       {Probl. Peredachi Inf.},
  note =          {(In Russian)},
  number =        {3},
  pages =         {103-107},
  title =         {Decoding Complexity Bound for Linear Block Codes},
  volume =        {25},
  year =          {1989},
  annote =        {A new complexity bound is derived for
                   maximum-likelihood decoding of linear block codes in
                   a memoryless q-ary symmetric channel. The bound is
                   the best among all known bounds in the entire range
                   of code rates.},
  url =           {http://mi.mathnet.ru/eng/ppi665},
}

@article{Coffey-Goodman-1990,
  author =        {Coffey, J. T. and Goodman, R. M.},
  journal =       {IEEE Trans. Info. Theory},
  month =         {Sep},
  number =        {5},
  pages =         {1031 -1037},
  title =         {The complexity of information set decoding},
  volume =        {36},
  year =          {1990},
  abstract =      {Information set decoding is an algorithm for decoding
                   any linear code. Expressions for the complexity of
                   the procedure that are logarithmically exact for
                   virtually all codes are presented. The expressions
                   cover the cases of complete minimum distance decoding
                   and bounded hard-decision decoding, as well as the
                   important case of bounded soft-decision decoding. It
                   is demonstrated that these results are vastly better
                   than those for the trivial algorithms of searching
                   through all codewords or through all syndromes, and
                   are significantly better than those for any other
                   general algorithm currently known. For codes over
                   large symbol fields, the procedure tends towards a
                   complexity that is subexponential in the symbol size},
  doi =           {10.1109/18.57202},
  issn =          {0018-9448},
}

@unpublished{Cuellar-etal-2020,
  author =        {M. P. Cu{\'e}llar and G{\'o}mez-Torrecillas, J. and
                   F. J. Lobillo and G. Navarro},
  note =          {unpublished},
  title =         {Genetic algorithms with permutation-based
                   representation for computing the distance of linear
                   codes},
  year =          {2020},
  annote =        {Finding the minimum distance of linear codes is an
                   NP-hard problem. Traditionally, this computation has
                   been addressed by means of the design of algorithms
                   that find, by a clever exhaustive search, a linear
                   combination of some generating matrix rows that
                   provides a codeword with minimum weight. Therefore,
                   as the dimension of the code or the size of the
                   underlying finite field increase, so it does
                   exponentially the run time. In this work, we prove
                   that, given a generating matrix, there exists a
                   column permutation which leads to a reduced row
                   echelon form containing a row whose weight is the
                   code distance. This result enables the use of
                   permutations as representation scheme, in contrast to
                   the usual discrete representation, which makes the
                   search of the optimum polynomial time dependent from
                   the base field. In particular, we have implemented
                   genetic and CHC algorithms using this representation
                   as a proof of concept. Experimental results have been
                   carried out employing codes over fields with two and
                   eight elements, which suggests that evolutionary
                   algorithms with our proposed permutation encoding are
                   competitive with regard to existing methods in the
                   literature. As a by-product, we have found and
                   amended some inaccuracies in the MAGMA Computational
                   Algebra System concerning the stored distances of
                   some linear codes.},
}

@misc{nist-mm-format,
  howpublished =  {online},
  note =          {Accessed on May 27, 2019},
  title =         {Matrix Market exchange formats},
  year =          {2013},
  url =           {https://math.nist.gov/MatrixMarket/formats.html},
}

@unpublished{Hastings-Haah-ODonnell-2020,
  author =        {Matthew B. Hastings and Jeongwan Haah and
                   {O'Donnell}, Ryan},
  note =          {unpublished},
  title =         {Fiber Bundle Codes: Breaking the {$N^{1/2}\mathop{\rm
                   polylog}(N)$} barrier for quantum {LDPC} Codes},
  year =          {2020},
  annote =        {We present a quantum LDPC code family that has
                   distance $O(N^{3/5}/polylog(N))$ and
                   $\Theta(N^{3/5})$ logical qubits. This is the first
                   quantum LDPC code construction which achieves
                   distance greater than $N^{1/2}polylog(N)$. The
                   construction is based on generalizing the homological
                   product of codes to a fiber bundle.},
}

@unpublished{Panteleev-Kalachev-2020,
  author =        {Pavel Panteleev and Gleb Kalachev},
  note =          {unpublished},
  title =         {Quantum {LDPC} Codes with Almost Linear Minimum
                   Distance},
  year =          {2020},
  annote =        {We give a construction of quantum LDPC codes of
                   dimension Θ(logN) and distance Θ(N/logN) as the
                   code length N→∞. Using a product of chain
                   complexes this construction also provides a family of
                   quantum LDPC codes of distance Ω(N1−α/2/logN) and
                   dimension Ω(NαlogN), where 0≤α<1. We also
                   introduce and study a new operation called lifted
                   product, which naturally generalizes the product
                   operations for quantum codes and chain complexes.},
}

@unpublished{Breuckmann-Eberhardt-2020,
  author =        {Nikolas P. Breuckmann and Jens N. Eberhardt},
  note =          {unpublished},
  title =         {Balanced Product Quantum Codes},
  year =          {2020},
  annote =        {This work provides the first explicit and non-random
                   family of [[N,K,D]] LDPC quantum codes which encode
                   K∈Θ(N45) logical qubits with distance D∈Ω(N35).
                   The family is constructed by amalgamating classical
                   codes and Ramanujan graphs via an operation called
                   balanced product. Recently, Hastings-Haah-O'Donnell
                   and Panteleev-Kalachev were the first to show that
                   there exist families of LDPC quantum codes which
                   break the polylog(N)N−−√ distance barrier.
                   However, their constructions are based on
                   probabilistic arguments which only guarantee the code
                   parameters with high probability whereas our bounds
                   hold unconditionally. Further, balanced products
                   allow for non-abelian twisting of the check matrices,
                   leading to a construction of LDPC quantum codes that
                   can be shown to have K∈Θ(N) and that we conjecture
                   to have linear distance D∈Θ(N).},
}

